---
title: Key Capacity Scaling Indicators
owner: PCF Metrics Platform Monitoring
---

This topic describes key capacity scaling indicators that operators monitor to determine when
they need to scale their Pivotal Cloud Foundry (PCF) deployments.

Pivotal provides these indicators to operators as general guidance for capacity scaling. 
Each indicator is based on platform metrics from different components. 
This guidance is applicable to most PCF v1.12 deployments.
Pivotal recommends that operators fine-tune the suggested alert thresholds 
by observing historical trends for their deployments. 

##<a id="cell"></a>Diego Cell Capacity Scaling Indicators

There are three key capacity scaling indicators recommended for Diego cell:

+ [Diego Cell Memory Capacity](#cell-memory) is a measure of the percentage of remaining memory capacity
+ [Diego Cell Disk Capacity](#cell-disk) is a measure of the percentage of remaining disk capacity
+ [Diego Cell Container Capacity](#cell-container) is a measure of the percentage of remaining container capacity

###<a id="cell-memory"></a>Diego Cell Memory Capacity

<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingMemory / rep.CapacityTotalMemory<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
     <td> Percentage of remaining memory capacity for a given cell. Monitor this derived metric across all cells in a deployment.
<br><br>
      The metric <code>rep.CapacityRemainingMemory</code> indicates the remaining amount in MiB of memory available for this cell to allocate to containers.<br>
      The metric <code>rep.CapacityTotalMemory</code> indicates the total amount in MiB of memory available for this cell to allocate to containers.</td>
 </tr>
   <tr>
      <th>Purpose</th>
      <td>A best practice deployment of Cloud Foundry includes three availability zones (AZs).  
          For these types of deployments, Pivotal recommends that you have enough capacity to suffer failure of an entire AZ.
          <br><br>
          The <em>Recommended threshold</em> assumes a three AZ configuration. Adjust the threshold percentage if you have more or less AZs. 
      </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td>&lt; avg(35%)</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>      
      Scale up your Diego Cells 
      </td>
   </tr>
    <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: Firehose<br>
           <strong>Type</strong>: Gauge (%)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:diego_cells
      </td>
   </tr>
</table>

###<a id="cell-disk"></a>Diego Cell Disk Capacity

<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingDisk / rep.CapacityTotalDisk<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td> Percentage of remaining disk capacity for a given cell. Monitor this derived metric across all cells in a deployment.
<br><br>
      The metric <code>rep.CapacityRemainingDisk</code> indicates the remaining amount in MiB of disk available for this cell to allocate to containers.<br> 
      The metric <code>rep.CapacityTotalDisk</code> indicates the total amount in MiB of disk available for this cell to allocate to containers.</td>
 </tr>
   <tr>
       <th>Purpose</th>
      <td>A best practice deployment of Cloud Foundry includes three availability zones (AZs).  
          For these types of deployments, Pivotal recommends that you have enough capacity to suffer failure of an entire AZ.
          <br><br>
          The <em>Recommended threshold</em> assumes a three AZ configuration. Adjust the threshold percentage if you have more or less AZs.
      </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td>&lt; avg(35%)</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>      
      Scale up your Diego Cells 
      </td>
   </tr>
    <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: Firehose<br>
           <strong>Type</strong>: Gauge (%)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:diego_cells
      </td>
   </tr>
</table>

###<a id="cell-container"></a>Diego Cell Container Capacity

<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingContainers / 
       rep.CapacityTotalContainers<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td> Percentage of remaining container capacity for a given cell. Monitor this derived metric across all cells in a deployment.
      <br><br>
      The metric <code>rep.CapacityRemainingContainers</code> indicates the remaining number of containers this cell can host.<br>
      The metric  by <code>rep.CapacityTotalContainer</code> indicates the total number of containers this cell can host.</td>
   <tr>
      <th>Purpose</th>
      <td>A best practice deployment of Cloud Foundry includes three availability zones (AZs).  
          For these types of deployments, Pivotal recommends that you have enough capacity to suffer failure of an entire AZ.
          <br><br>
          The <em>Recommended threshold</em> assumes a three AZ configuration. Adjust the threshold percentage if you have more or less AZs.
      </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td>&lt; avg(35%)</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>
      Scale up your Diego Cells 
     </td>
   </tr>
    <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: Firehose<br>
           <strong>Type</strong>: Gauge (%)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:diego_cells
      </td>
   </tr>
</table>

## <a id="doppler-server"></a> Firehose Performance Scaling Indicator

There is one key capacity scaling indicator recommended for Firehose performance. 

### <a id="firehose-loss-rate"></a> Firehose Loss Rate

<table>
  <tr>
      <th colspan="2" style="text-align: center;">(DopplerServer.doppler.shedEnvelopes +  loggregator.doppler.dropped) / (DopplerServer.listeners.totalReceivedMessageCount + loggregator.doppler.ingress)</th>
  </tr>
   <tr>
      <th width="25">Description</th>
      <td> This derived value represents the Firehose loss rate, or the total messages dropped as a percentage of the total message throughput.
           Total messages include the combined stream of logs from all apps and the metrics data from Cloud Foundry components.
       </td>
   </tr>
   <tr>
      <th>Purpose</th>
      <td>Excessive dropped messages can indicate the Dopplers and/or Traffic Controllers are not processing messages fast enough.
        <br><br>
        The recommended scaling indicator is to look at the total dropped as a percentage of the total throughput
        and scale if the derived loss rate value grows greater than <code>0.1</code>.
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td><strong>Scale indicator</strong>: &ge; 0.1</br>
      If alerting:<br>
      <strong>Yellow warning</strong>: &ge; 0.05<br>
      <strong>Red critical</strong>: &ge; 0.1</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>Scale up the Firehose Traffic Controllers and Dopplers. 
      </td>
   </tr>
   <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: Firehose<br>
           <strong>Type</strong>: Gauge (float)<br>
           <strong>Frequency</strong>: Base metrics are emitted every 5 s<br>
           <strong>Applies to</strong>: cf:doppler<br>
      </td>
   </tr>
</table>

## <a id="scalable-syslog"></a> Scalable Syslog Performance Scaling Indicators
There are three key capacity scaling indicators recommended for Scalable Syslog performance. 

<p class="note"><strong>Note</strong>: These scalable syslog scaling indicators are only relevant
  if your deployment contains apps using the scalable syslog drain binding feature.</p>

### <a id="syslog-adapter-ksi"></a> Adapter Loss Rate

<table>
  <tr>
      <th colspan="2" style="text-align: center;">scalablesyslog.adapter.dropped 
                       / scalablesyslog.adapter.ingress</th>
  </tr>
   <tr>
      <th width="25">Description</th>
       <td>The loss rate of the scalable syslog adapters, that is, the total messages dropped 
          as a percentage of the total traffic coming through the <a href="../loggregator/architecture.html">scalable syslog adapters</a>.
          Total messages include only logs for bound applications.
       <br><br>
			This loss rate is specific to the scalable syslog adapters and does not impact the Firehose loss rate.
                        For example, you can suffer lossiness in syslog while not suffering any lossiness in the Firehose.
      </td>
   </tr>
   <tr>
      <th>Purpose</th>
      <td>Indicates that the syslog drains are not keeping up with the number of logs 
                            that a syslog-drain-bound app is producing.
                            This likely means that the syslog-drain consumer is failing to keep up with the incoming log volume.         
           <br><br>
        The recommended scaling indicator is to look at the maximum per minute loss rate over a 5-minute window and scale if the derived loss rate value grows greater than <code>0.1</code>.
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td><strong>Scale indicator</strong>: &ge; 0.1</br>
      If alerting:<br>
      <strong>Yellow warning</strong>: &ge; 0.01<br>
      <strong>Red critical</strong>: &ge; 0.1</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>Performance test your syslog server, review the logs of the syslog consuming system for intake
          and other performance issues that indicate a need to scale the consuming system.
      </td>
   </tr>
   <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: Firehose<br>
           <strong>Type</strong>: Counter (Integer)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:scalablesyslog<br>
      </td>
   </tr>
</table>

### <a id="syslog-rlp-ksi"></a> Reverse Log Proxy Loss Rate

<table>
  <tr>
      <th colspan="2" style="text-align: center;">loggregator.rlp.dropped / loggregator.rlp.ingress</th>
  </tr>
   <tr>
      <th width="25">Description</th>
       <td>The loss rate of the reverse log proxies (RLP), that is, the total messages dropped 
           as a percentage of the total traffic coming through the <a href="../loggregator/architecture.html">reverse log proxy</a>.
           Total messages include only logs for bound applications.
       <br><br>
			This loss rate is specific to the scalable syslog RLP and does not impact the Firehose loss rate.
                        For example, you can suffer lossiness in syslog while not suffering any lossiness in the Firehose.
      </td>
   </tr>
   <tr>
      <th>Purpose</th>
      <td>Excessive dropped messages can indicate that the RLP is overloaded and that the Traffic Controllers need to be scaled.
         <br><br>
        The recommended scaling indicator is to look at the maximum per minute loss rate over a 5-minute window 
        and scale if the derived loss rate value grows greater than <code>0.1</code>.
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td><strong>Scale indicator</strong>: &ge; 0.1</br>
      If alerting:<br>
      <strong>Yellow warning</strong>: &ge; 0.01<br>
      <strong>Red critical</strong>: &ge; 0.1</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>Scale up the number of traffic controller instances to further balance log load.
      </td>
   </tr>
   <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: Firehose<br>
           <strong>Type</strong>: Counter (Integer)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:scalablesyslog<br>
      </td>
   </tr>
</table>

### <a id="scalable-syslog-ksi"></a> Scalable Syslog Drain Bindings Count

<table>
  <tr>
      <th colspan="2" style="text-align: center;">scalablesyslog.scheduler.drains</th>
  </tr>
   <tr>
      <th width="25">Description</th>
       <td>The number of scalable syslog drain bindings.<br>
       </td>
   </tr>
   <tr>
      <th>Purpose</th>
      <td>Each scalable syslog adapter can handle approximately 250 drain bindings. 
          The recommended initial configuration is a minimum of two scalable syslog adapters
          (to handle approximately 500 drain bindings). 
          A new adapter instance should be added for each 250 additional drain bindings.
        <br><br>
        Therefore, the recommended initial scaling indicator is 450 (as a maximum value over a 1-hr window).
        This indicates the need to scale up to three adapters from the initial two-adapter configuration. 
         
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td><strong>Scale indicator</strong>: &ge; 450<br>
         Consider this threshold to be dynamic.
         Adjust the threshold to the PCF deployment as adoption of scalable syslog increases or decreases.</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>Increase the number of Scalable Syslog Adapter VMs in the <strong>Resource Config</strong> pane of the Elastic Runtime tile.
      </td>
   </tr>
   <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: Firehose<br>
           <strong>Type</strong>: Gauge (float)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:scalablesyslog<br>
      </td>
   </tr>
</table>

## <a id="Router"></a> Router Performance Scaling Indicator
There is one key capacity scaling indicator recommended for Router performance. 

###<a id="system.cpu.user"></a>Router VM CPU Utilization

<table>
  <tr>
      <th colspan="2" style="text-align: center;">system.cpu.user of Gorouter VM(s)</th>
  </tr>
  <tr>
      <th>Description</th>
      <td>CPU utilization of the Gorouter VM(s)</td>
  </tr>
  <tr>
      <th>Purpose</th>
      <td>High CPU utilization of the Gorouter VMs can increase latency and cause throughput, or requests per/second, to level-off.
          Pivotal recommends keeping the CPU utilization within a maximum range of 60-70%  for best Gorouter performance. 
      <br><br>
      If you want to increase throughput capabilities while also keeping latency low, Pivotal recommends scaling the Gorouter 
      while continuing to ensure that CPU utilization does not exceed the maximum recommended  range. 
          </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td><strong>Scale indicator</strong>: &ge; 60%<br>
      If alerting:<br>
      <strong>Yellow warning</strong>: &ge; 60%<br>
      <strong>Red critical</strong>: &ge; 70%<br>
      </td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>Resolve high utilization by scaling the Gorouters horizontally or vertically 
          by editing the <b>Router</b> VM in the <b>Resource Config</b> pane of the Elastic Runtime tile. 
      </td>
   </tr>
   <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
           <strong>Type</strong>: Gauge (float)<br>
           <strong>Frequency</strong>: Emitted every 60 s<br>
           <strong>Applies to</strong>: cf:router<br>
      </td>
   </tr>
</table>

##<a id="systemdiskpersist"></a>NFS/WebDAV Backed Blobstore

There is one key capacity scaling indicator for external S3 external storage.

<p class="note"><strong>Note</strong>: This metric is only relevant if your deployment does not use 
                        an external S3 repository for external storage with no capacity constraints.</p>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>system.disk.persistent.percent of NFS server VM(s)<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td><em>If applicable</em>: Monitor the percentage of persistent disk used on the VM for the NFS Server job.
           </tr>
   <tr>
       <th>Purpose</th>
       <td>If you do not use an external S3 repository for external storage with no capacity constraints,
           you must monitor the PCF object store to push new app and buildpacks.
     <br><br>
     If you use an internal NFS/WebDAV backed blobstore, consider scaling the persistent disk when it reaches 75% capacity.
     </td>
   </tr>
   <tr>
      <th>Recommended thresholds</th>
      <td>&ge; 75%</td>
   </tr>
   <tr>
      <th>How to scale</th>
      <td>
      Give your NFS Server additional persistent disk resources.  
      </td>
   </tr>
    <tr>
      <th>Additional details</th>
      <td> <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
           <strong>Type</strong>: Gauge (%)<br>
           <strong>Applies to</strong>: cf:nfs_server<br>
      </td>
   </tr>  
</table>
