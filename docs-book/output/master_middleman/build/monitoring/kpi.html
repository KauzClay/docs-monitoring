<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Always force latest IE rendering engine or request Chrome Frame -->
  <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">

  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,400italic,400,600' rel='stylesheet' type='text/css'>
  <!-- Use title if it's in the page YAML frontmatter -->
  <title>
      Key Performance Indicators |
    
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="/stylesheets/all.css" rel="stylesheet" media="screen, print" />
  <link href="/stylesheets/print.css" rel="stylesheet" media="print" />
  <link href='/images/favicon.ico' rel='shortcut icon'>

  <script src="/javascripts/all.js"></script>
  <script>
//Global Google Search
(function() {
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//www.google.com/cse/cse.js?cx=' + '';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
})();
</script>

<script type="text/javascript">
  if (window.location.host === 'hello') {
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', '']);
    _gaq.push(['_setDomainName', '']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script');
      ga.type = 'text/javascript';
      ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(ga, s);
    })();
  }
</script>

<script type="text/javascript">
  var blackList = ['acceptance'];

  blackList.forEach(function(blackListedEnv) {
    if (document.URL.indexOf(blackListedEnv) > -1 && blackListedEnv != "") {
      $('head').append('<meta name="hashtag" content="PivotalMoment">');
    }
  });
</script>

<!-- CrazyEgg Tracking Script -->
<script type="text/javascript">
  setTimeout(function() {
    var a = document.createElement("script");
    var b = document.getElementsByTagName("script")[0];
    a.src = document.location.protocol + "//script.crazyegg.com/pages/scripts/0020/8294.js?" + Math.floor(new Date().getTime() / 3600000);
    a.async = true;
    a.type = "text/javascript";
    b.parentNode.insertBefore(a, b)
  }, 1);
</script>

<script type="text/javascript">
  (function() {
    var didInit = false;

    function initMunchkin() {
      if (didInit === false) {
        didInit = true;
        Munchkin.init('625-IUJ-009');
      }
    }

    var s = document.createElement('script');
    s.type = 'text/javascript';
    s.async = true;
    s.src = document.location.protocol + '//munchkin.marketo.net/munchkin.js';
    s.onreadystatechange = function() {
      if (this.readyState == 'complete' || this.readyState == 'loaded') {
        initMunchkin();
      }
    };
    s.onload = initMunchkin;
    document.getElementsByTagName('head')[0].appendChild(s);
  })();
</script>

</head>

<body class="monitoring monitoring_kpi has-subnav">

<div class="viewport">
  <div class='wrap'>
    <script type="text/javascript">
      document.domain = "";
    </script>

       
    <header class="header header-layout">
      <h1 class="logo">
        <a href="/">Default Book Title</a>
      </h1>
      
      <div class="header-links js-bar-links">
        <div class="btn-menu" data-behavior="MenuMobile"></div>
        
        <div class="header-item">
          
        </div>
        <!--Default search-->
<!--If book needs something different, add a book-search partial to the book's layouts folder-->
<div class="header-item searchbar js-searchbar">
  <a class="search-icon" data-behavior="Search"></a>
  <div class="search-input">
    <div class="search-input-inner" id="docs-search">
        <gcse:searchbox></gcse:searchbox>
    </div>
  </div>
</div>

      </div>
    </header>



    <div class="container">

      <!--googleoff: index-->
      <div id="sub-nav" class="js-sidenav nav-container" role="navigation">
  <a class="sidenav-title" data-behavior="SubMenuMobile">
    Doc Index
  </a>
  <div class="nav-content">
    <ul>
      <li>
        <a id='home-nav-link' href="/monitoring/kpi.html">Key Performance Indicators</a>
      </li>

      <li>
        <a href="/monitoring/key-cap-scaling.html">Key Capacity Scaling Indicators</a>
      </li>
    </ul>
  </div>
</div><!--end of sub-nav-->

      <!--googleon: index-->

      <main class="content content-layout" id="js-content" role="main">
        <a id="top"></a>
        
          <h1 class="title-container" >
    Key Performance Indicators
  </h1>

          <div id="js-quick-links" >
            <div class="quick-links"><ul>
<li>
<a href="#auctioneer">Diego Auctioneer Metrics</a><ul>
<li><a href="#AuctioneerLRPAuctionsFailed">Auctioneer AI Placement Failures</a></li>
<li><a href="#AuctioneerFetchStatesDuration">Auctioneer Time to Fetch Cell State</a></li>
<li><a href="#AuctioneerLRPAuctionsStarted">Auctioneer App Instance Starts</a></li>
<li><a href="#AuctioneerTaskAuctionsFailed">Auctioneer Task Placement Failures</a></li>
</ul>
</li>
<li>
<a href="#bbs">Diego BBS Metrics</a><ul>
<li><a href="#ConvergenceLRPDuration">BBS Time to Run LRP Convergence</a></li>
<li><a href="#RequestLatency">BBS Time to Handle Requests</a></li>
<li><a href="#cc-diego-sync">Cloud Controller and Diego in Sync</a></li>
<li><a href="#LRPsExtra">More App Instances Than Expected</a></li>
<li><a href="#LRPsMissing">Fewer App Instances Than Expected</a></li>
<li><a href="#bbs.CrashedActualLRPs">Crashed App Instances</a></li>
<li><a href="#1hraverageofbbs.LRPsRunning">Running App Instances, Rate of Change</a></li>
</ul>
</li>
<li>
<a href="#cell">Diego Cell Metrics</a><ul>
<li><a href="#rep.CapacityRemainingMemory">Remaining Memory Available &mdash; Cell Memory Chunks Available</a></li>
<li><a href="#rep.CapacityRemainingMemory2">Remaining Memory Available &mdash; Overall Remaining Memory Available</a></li>
<li><a href="#CapacityRemainingDisk">Remaining Disk Available</a></li>
<li><a href="#RepBulkSyncDuration">Cell Rep Time to Sync</a></li>
<li><a href="#UnhealthyCell">Unhealthy Cells</a></li>
</ul>
</li>
<li>
<a href="#nsync_bulker">Diego nsync_bulker Metrics</a><ul><li><a href="#DesiredLRPSyncDuration">Nsync-bulker Time to Sync</a></li></ul>
</li>
<li>
<a href="#route_emitter">Diego router-emitter Metrics</a><ul>
<li><a href="#RouteEmitterSyncDuration">Route Emitter Time to Sync</a></li>
<li><a href="#ConsulDownMode">Consul Up or Down</a></li>
</ul>
</li>
<li>
<a href="#gorouter">Gorouter Metrics</a><ul>
<li><a href="#total_requests">Router Throughput</a></li>
<li><a href="#latency">Router Handling Latency</a></li>
<li><a href="#mssincelastregistryupdate">Time Since Last Route Register Received</a></li>
<li><a href="#bad_gateways">Router Error: 502 Bad Gateway</a></li>
<li><a href="#responses.5xx">Router Error: Server Error</a></li>
<li><a href="#total_routes">Number of Gorouter Routes Registered</a></li>
</ul>
</li>
<li>
<a href="#doppler-server">Doppler Server Metrics</a><ul>
<li><a href="#listeners.receivedEnvelopes">Firehose Throughput</a></li>
<li><a href="#deriveddopplerserver.doppler">Firehose Dropped Messages</a></li>
</ul>
</li>
<li>
<a href="#bosh">System (BOSH) Metrics</a><ul>
<li><a href="#healthy">VM Health</a></li>
<li><a href="#mem.percent">VM Memory Used</a></li>
<li><a href="#disk.system.percent">VM Disk Used</a></li>
<li><a href="#disk.ephemeral.percent">VM Ephemeral Disk Used</a></li>
<li><a href="#disk.persistent.percent">VM Persistent Disk Used</a></li>
<li><a href="#cpu.user">VM CPU Utilization</a></li>
</ul>
</li>
</ul></div>
          </div>
        <div class="to-top" id="js-to-top">
          <a href="#top" title="back to top"></a>
        </div>
        <style>
    .note.warning {
        background-color: #fdd;
        border-color: #fbb
    }

    .note.warning:before {
        color: #f99;
     }
</style>

<p>This topic describes Key Performance Indicators (KPIs) that operators may want to monitor with their Pivotal Cloud Foundry (PCF) deployment to help ensure it is in a good operational state.</p>

<p>The following PCF v1.10 KPIs are provided for operators to give general guidance on monitoring a PCF deployment using platform component and system (BOSH) metrics. 
Although many metrics are emitted from the platform, the following PCF v1.10 KPIs are 
high-signal-value metrics that can indicate emerging platform issues. </p>

<p>This alerting and response guidance has been shown to apply to most deployments. 
Pivotal recommends that operators continue to fine-tune the alert measures to their deployment 
by observing historical trends. 
Pivotal also recommends that operators expand beyond this guidance and create new, deployment-specific monitoring
metrics, thresholds, and alerts based on learning from their deployments.</p>

<h2><a id="auctioneer"></a> Diego Auctioneer Metrics</h2>

<h3><a id="AuctioneerLRPAuctionsFailed"></a>Auctioneer AI Placement Failures</h3>

<table>
   <tr><th colspan="2" style="text-align: center;"><br> auctioneer.AuctioneerLRPAuctionsFailed<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The number of Long Running Process (LRP) instances that the auctioneer failed to place on Diego cells. This metric is cumulative over the lifetime of the auctioneer job.<br><br>
   
      <strong>Use</strong>: This metric can indicate that PCF is out of container space or that there is a lack of resources within your environment. This indicator also increases when the LRP is requesting an isolation segment, volume drivers, or a stack that is unavailable, either not deployed or lacking in sufficient resources to accept the work. 
  <br><br> 
      This error is most common due to capacity issues, for example, if cells do not have 
      enough resources or if cells are going back and forth between a healthy and unhealthy state.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: Counter, integer<br>
      <strong>Frequency</strong>: During each auction<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Per minute delta averaged over a 5-minute window</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 0.5<br>
      <strong>Red critical</strong>: &ge; 1</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         <ol>
        <li>In order to best determine the root cause, examine the Auctioneer logs. Depending on the specific error and/or resource constraint, you may also find a failure reason in the Cloud Controller (CC) API.</li> 
            <li>Investigate the health of your Diego cells to determine if they
            are the resource type causing the problem.</li> 
            <li>Consider scaling additional cells using Ops Manager.</li>
            <li>If scaling cells does not solve the problem, pull Diego brain logs and BBS node logs
             and contact Pivotal Support telling them that LRP auctions are failing.</li>
         </ol>
      </td>
   </tr>
</table>

<h3><a id="AuctioneerFetchStatesDuration"></a>Auctioneer Time to Fetch Cell State</h3>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerFetchStatesDuration<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td> Time in ns that the auctioneer took to fetch state from all the Diego cells when running its auction.<br><br>

      <strong>Use</strong>: Indicates how the cells themselves are performing. 
      Alerting on this metric helps alert that app staging requests to Diego may be failing.

      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: Gauge, integer in ns<br>
      <strong>Frequency</strong>: During event, during each auction<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td> Maximum over the last 5 minutes divided by 1,000,000,000</td>
   </tr>
   <tr>
         <th>Recommended alert thresholds</th>
         <td><strong>Yellow warning</strong>: &ge; 5 s<br>
         <strong>Red critical</strong>: &ge; 10 s</td>
   </tr>
   <tr>
      <th>Recommended response</th>
        <td>
          <ol>
             <li>Check the health of the cells by reviewing the logs and looking for errors.</li>
             <li>Review IaaS console metrics.</li>
             <li>Pull Diego brain logs and cell logs and contact Pivotal Support telling them that fetching cell states is taking too long.</li>
          </ol>
       </td>
   </tr>
</table>

<h3><a id="AuctioneerLRPAuctionsStarted"></a>Auctioneer App Instance Starts</h3>

<table> 
   <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerLRPAuctionsStarted<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The number of LRP instances that the auctioneer successfully placed on Diego cells. This metric is cumulative over the lifetime of the auctioneer job. 
                        <br><br>
                <strong>Use</strong>: Provides a sense of running system activity levels in your environment. 
                Can also give you a sense of how many app instances have been started over time.
                The recommended measurement, below, can help indicate a significant amount of container churn. 
                However, for capacity planning purposes, it is more helpful to observe deltas over a long time window. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: During event, during each auction<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Per minute delta averaged over a 5-minute window</td>
</tr>
<tr>
<th>Recommended alert thresholds</th>
<td><strong>Yellow warning</strong>: Dynamic<br>
<strong>Red critical</strong>: Dynamic</td>
</tr>
<tr>
<th>Recommended response</th>
<td>
When observing a significant amount of container churn, do the following:<br><br>
    <ol>
    <li>Look to eliminate explainable causes of temporary churn, such as a deployment or increased developer activity.</li>
    <li>If container churn appears to continue over an extended period, pull logs from the Diego Brain and BBS node before contacting Pivotal support.</li>
    </ol>       
When observing extended periods of high or low activity trends, scale up or down CF components as needed.

    </td>
</tr>
</table>

<h3><a id="AuctioneerTaskAuctionsFailed"></a> Auctioneer Task Placement Failures</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>auctioneer.AuctioneerTaskAuctionsFailed<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The number of Tasks that the auctioneer failed to place on Diego cells. This metric is cumulative over the lifetime of the auctioneer job. 
                        <br><br>
                <strong>Use</strong>: Failing Task auctions indicate a lack of resources 
                        within your environment and that you likely need to scale. 
                        This indicator also increases when the Task is requesting an isolation segment, volume drivers, 
                        or a stack that is unavailable, either not deployed or lacking in sufficient resources to accept the work.
                <br><br>
            This error is most common due to capacity issues, for example, if cells do not have enough resources or if cells are going back and forth between a healthy and unhealthy state. 
            <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Float)<br>
                 <strong>Frequency</strong>: During event, during each auction<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Per minute delta averaged over a 5-minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 0.5 <br>
                <strong>Red critical</strong>: &ge; 1</td>
        </tr>
<tr>
<th>Recommended response</th>
<td>
    <ol>
        <li>In order to best determine the root cause, examine the Auctioneer logs. Depending on the specific error and/or resource constraint, you may also find a failure reason in the CC API.</li>
    <li>Investigate the health of Diego cells.</li>
    <li>Consider scaling additional cells using Ops Manager.</li>
    <li>If scaling cells does not solve the problem, 
                        then pull Diego brain logs and BBS logs for troubleshooting, 
                        and contact Pivotal Support telling them that Task auctions are failing.</li>
    </ol>
    </td>
</tr>
</table>

<h2><a id="bbs"></a> Diego BBS Metrics</h2>

<h3><a id="ConvergenceLRPDuration"></a>BBS Time to Run LRP Convergence</h3>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.ConvergenceLRPDuration<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Time in ns that the BBS took to run its LRP convergence pass. 
          <br><br>
          <strong>Use</strong>: If the convergence run begins taking too long, 
          apps or Tasks may be crashing without restarting. 
          This symptom can also indicate loss of connectivity to the BBS database.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in ns)<br>
          <strong>Frequency</strong>: During event,
                  every 30 seconds when LRP convergence runs, emission should be near-constant on a running deployment
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>15-minute maximum divided by 1,000,000,000</td>
   </tr>
      <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 10 s<br>
          <strong>Red critical</strong>: &ge; 20 s</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Check BBS logs for errors.</li>
              <li>Try vertically scaling the BBS VM resources up. 
              For example, add more CPUs/memory depending on its <code>system.cpu</code>/<code>system.memory</code> metrics.</li>
              <li>If that does not solve the issue, pull the BBS logs and contact Pivotal Support 
                  for additional troubleshooting.
          </ol>
          </td>
   </tr>
</table>

<h3><a id="RequestLatency"></a>BBS Time to Handle Requests</h3>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.RequestLatency<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Time in ns that the BBS took to handle requests, aggregated across all its API endpoints. 
          <br><br>
          <strong>Use</strong>: If this metric rises, the PCF API is slowing. 
          Response to certain <code>cf</code> commands is slow if request latency is high.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in ns)<br>
          <strong>Frequency</strong>: During event, when the BBS API handles requests, 
                  emission should be near-constant on a running deployment<br>
    </tr>
    <tr>
      <th>Recommended measurement</th>
      <td>15 minute maximum divided by 1,000,000,000</td>
    </tr>
    <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &ge; 5 s<br>
          <strong>Red critical</strong>: &ge; 10 s</td>
    </tr>
    <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Check CPU and memory statistics in Ops Manager.</li>
              <li>Check BBS logs for faults and errors that can indicate issues with BBS.</li>
              <li>Try scaling the BBS VM resources up. For example, add more CPUs/memory 
                  depending on its <code>system.cpu</code>/<code>system.memory</code> metrics.</li>
              <li>If the above steps do not solve the issue, then collect a sample of the cell logs 
                  from the BBS VMs and contact Pivotal Support to troubleshoot further.
          </ol>
          </td>
    </tr>
</table>

<h3><a id="cc-diego-sync"></a>Cloud Controller and Diego in Sync</h3>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>bbs.Domain.cf-apps<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Indicates if the <code>cf-apps</code> Domain is up-to-date, meaning that CF App requests from Cloud Controller are synchronized to Diego desired AIs (<code>bbs.LRPsDesired</code>) for execution. 
            <ul>
              <li><code>1</code> means <code>cf-apps</code> Domain is up-to-date</li>
              <li>No data received means <code>cf-apps</code> Domain is not up-to-date</li>
            </ul>
          <strong>Use</strong>: If the <code>cf-apps</code> Domain does not stay up-to-date, changes requested in the Cloud Controller are not guaranteed to propagate throughout the system. If the Cloud Controller and Diego are out of sync, then apps running could vary from those desired.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Float)<br>
          <strong>Frequency</strong>: 30 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over the last 5 minutes</code></td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
      <strong>Red critical</strong>: &lt; 1 </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Check the nysnc bulker logs and BBS logs.</li>
              <li>If the problem continues, pull Diego brain logs and BBS logs 
                  and contact Pivotal Support to say that the <code>cf-apps</code> domain is not being kept fresh.</li>
          </ol>
      </td>
   </tr>
</table>

<h3><a id="LRPsExtra"></a>More App Instances Than Expected</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.LRPsExtra<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that are no longer desired but still have a BBS record. 
                    When Diego wants to add more apps, the BBS sends a request to the auctioneer 
                    to spin up additional LRPs. LRPsExtra is the total number of LRP instances that are no longer desired but still have a BBS record. 
                        <br><br>
                <strong>Use</strong>: If Diego has more LRPs running than expected, 
                        there may be problems with the BBS.<br><br>
                        Deleting an app with many instances can temporarily spike this metric.  
                        However, a sustained spike in <code>bbs.LRPsExtra</code> is unusual and should be investigated.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>:  &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Review the BBS logs for proper operation or errors, looking for detailed error messages.</li>
                        <li>If the condition persists, pull the BBS logs and contact Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="LRPsMissing"></a>Fewer App Instances Than Expected</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.LRPsMissing<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that are desired but have no record in the BBS. When Diego wants to add more apps, the BBS sends a request to the auctioneer 
                        to spin up additional LRPs. LRPsMissing is the total number of LRP instances that are desired but have no BBS record. 
                        <br><br>
                <strong>Use</strong>: If Diego has less LRP running than expected, there may be problems with the BBS.<br><br> 
                        An app push with many instances can temporarily spike this metric. However, a sustained spike in <code>bbs.LRPsMissing</code> is unusual and should be investigated.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>: &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Review the BBS logs for proper operation or errors, looking for detailed error messages.</li>
                        <li>If the condition persists, pull the BBS logs and contact Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="bbs.CrashedActualLRPs"></a>Crashed App Instances</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>bbs.CrashedActualLRPs<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Total number of LRP instances that have crashed.
                <br><br>
                <strong>Use</strong>: Indicates how many instances in the deployment are in a crashed state. 
                An increase in <code>bbs.CrashedActualLRPs</code> can indicate several problems, 
                from a bad app with many instances associated, 
                to a platform issue that is resulting in app crashes. 
                Use this metric to help create a baseline for your deployment. 
                After you have a baseline, you can create a deployment-specific alert to notify of a spike in crashes above the trend line. 
                Tune alert values to your deployment. 
                <br><br>
                <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over last 5 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Look at the BBS logs for apps that are crashing 
                            and at the cell logs to see if the problem is with the apps themselves, rather than 
                            a platform issue.</li>
                        <li>Before contacting Pivotal Support, pull the BBS logs and, if particular apps are the problem,
                            pull the logs from their Diego cells too.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="1hraverageofbbs.LRPsRunning"></a>Running App Instances, Rate of Change</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>1hr average of bbs.LRPsRunning – prior 1hr average of bbs.LRPsRunning<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Rate of change in app instances being started or stopped on the platform. 
                    It is derived from <code>bbs.LRPsRunning</code>
                    and represents the total number of LRP instances that are running on Diego cells.
                    <br><br>
                    <strong>Use</strong>: Delta reflects upward or downward trend for app instances started or stopped. 
                    Helps to provide a picture of the overall growth trend of the environment for capacity planning. 
                    You may want to alert on delta values outside of the expected range.
                    <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: During event, 
                         emission should be constant on a running deployment<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>derived=(1-hour average of <code>bbs.LRPsRunning</code> – prior 1-hour average of <code>bbs.LRPsRunning</code>)</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                     Scale components as necessary.
                </td>
        </tr>
</table>

<h2><a id="cell"></a> Diego Cell Metrics</h2>

<h3><a id="rep.CapacityRemainingMemory"></a>Remaining Memory Available — Cell Memory Chunks Available</h3>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingMemory<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Remaining amount of memory in MiB available for this Diego cell to allocate to containers.
          <br><br>
          <strong>Use</strong>: Indicates the available cell memory. Insufficient cell memory can prevent pushing and scaling apps.
          <br><br>
          The strongest operational value of this metric is to understand a deployment&rsquo;s average app size and monitor/alert on ensuring that at least some cells have large enough capacity to accept standard app size pushes. For example, if pushing a 4GB app, Diego would have trouble placing that app if there is no one cell with sufficient capacity of 4GB or greater.
      <br><br>
          As an example, Pivotal Cloud Ops uses a standard of 4GB, and computes and monitors for the number of cells with at least 4GB free. When the number of cells with at least 4GB falls below a defined threshold, this is a scaling indicator alert to increase  capacity. This <em>free chunk</em> count threshold should be tuned to the deployment size and the standard size of apps being pushed to the deployment.<br><br>

          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in bytes)<br>
          <strong>Frequency</strong>: 60 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td><b>For alerting</b>: <ol>
            <li>Determine the size of a standard app in your deployment. This is the suggested value to calculate <em>free chunks</em> of Remaining Memory by.
          <li>Create a script/tool that can iterate through each Diego Cell and do the following: 
          <ol>
          <li>Pull the <code>rep.CapacityRemainingMemory</code> metric for each cell.</li>
          <li>Divide the values received by 1000 to get the value in Gigabytes (if desired threshold is GB-based).</li>
          <li>Compare recorded values to your minimum capacity threshold, and count the number of cells that have equal or greater than the desired amount of <em>free chunk</em> space.</li>    
           </ol>
     <li> Determine a desired scaling threshold based on the minimum amount of <em>free chunks</em> that are acceptable in this deployment given historical trends.</li>
     <li> Set an alert to indicate the need to scale cell memory capacity when the value falls below the desired threshold number.</li>
          </ol> 
    <b>For visualization purposes</b>:<br>
          Looking at this metric (<code>rep.CapacityRemainingMemory</code>) as a minimum value per cell has more informational value than alerting value. It can be an interesting heatmap visualization, showing average variance and/or density over time.   
</td>
  </tr>
  <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br>
          <strong>Red critical</strong>: Dynamic<br>
                 </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Assign more resources to the cells or assign more cells.</li>
                        <li>Scale additional Diego cells using Ops Manager.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="rep.CapacityRemainingMemory2"></a>Remaining Memory Available — Overall Remaining Memory Available</h3>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>rep.CapacityRemainingMemory<br>
       (Alternative Use)<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Remaining amount of memory in MiB available for this Diego cell to allocate to containers.
          <br><br>
          <strong>Use</strong>: Can indicate low memory capacity overall in the platform. 
          Low memory can prevent app scaling and new deployments. 
          The overall sum of capacity can indicate that you need to scale the platform.
          Observing capacity consumption trends over time helps with capacity planning.
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Gauge (Integer in bytes)<br>
          <strong>Frequency</strong>: 60 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>5 minute sum across all cells divided by 0.000001024</td>
  </tr>
  <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 32<br>
          <strong>Red critical</strong>: &le; 16</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
              <li>Assign more resources to the cells or assign more cells.</li>
              <li>Scale additional Diego cells via Ops Manager.</li>
          </ol>
                    </td>
        </tr>
</table>

<h3><a id="CapacityRemainingDisk"></a>Remaining Disk Available</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br> rep.CapacityRemainingDisk<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Remaining amount of disk in MiB available for this Diego cell to allocate to containers.
                    <br><br>
                    <strong>Use</strong>: Low disk capacity can prevent app scaling and new deployments. Because Diego staging Tasks can fail without at least 4&nbsp;GB free, 
                the recommended red threshold is based on the minimum disk capacity across the deployment
                falling below 4&nbsp;GB in the previous 5 minutes.
        <br><br>
        It can also be meaningful to assess how many chunks of free disk space are above a given threshold, similar to <code>rep.CapacityRemainingMemory</code>.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Integer in bytes)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Minimum over the last 5 minutes divided by 1024 (across all instances)</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &le; 6<br>
                <strong>Red critical</strong>:&le; 3.5 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Assign more resources to the cells or assign more cells.</li>
                        <li>Scale additional cells using Ops Manager.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="RepBulkSyncDuration"></a>Cell Rep Time to Sync</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>rep.RepBulkSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the Diego Cell Rep took to sync the ActualLRPs that it claimed with its actual garden containers. 
                        <br><br>
                <strong>Use</strong>: Sync times that are too high can indicate issues with the BBS.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ns)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 15 minutes divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 10 s<br>
                <strong>Red critical</strong>: &ge; 20 s </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Investigate BBS logs for faults and errors.</li>
                        <li>If a particular cell or cells appear problematic, 
                            pull logs for the cells and the BBS logs before contacting Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="UnhealthyCell"></a>Unhealthy Cells</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>rep.UnhealthyCell<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The Diego cell periodically checks its health against the garden backend. 
                    For Diego cells, <code>0</code> means healthy, and <code>1</code> means unhealthy.
                    <br><br>
                    <strong>Use</strong>: Set an alert for further investigation if 
                    multiple unhealthy Diego cells are detected in the given time window. 
                    If one cell is impacted, it does not participate in auctions, 
                    but end-user impact is usually low. If multiple cells are impacted, 
                    this can indicate a larger problem with Diego.
                 <br><br>
        Suggested alert threshold based on multiple unhealthy cells in the given time window.
        <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float, 0-1)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 5 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em><br>
                <strong>Red critical</strong>: &gt; 1 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Investigate Diego cell servers for faults and errors.</li>
                        <li>If a particular cell or cells appear problematic, 
                            pull logs for that cell, as well as the BBS logs before contacting Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h2><a id="nsync_bulker"></a> Diego nsync_bulker Metrics</h2>

<h3><a id="DesiredLRPSyncDuration"></a>Nsync-bulker Time to Sync</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>nsync_bulker.DesiredLRPSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the nsync-bulker took to synchronize CF apps and Diego DesiredLRPs.
                    <br><br>
                    <strong>Use</strong>: Cloud Controller and Diego brain should be kept synchronized. 
                    This symptom can indicate that the BBS database is unhealthy. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ns)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 15 minutes 
                    divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 10 s<br>
                <strong>Red critical</strong>: &ge; 20 s</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Investigate BBS metrics and logs and Cloud Controller Bridge logs for errors.
                    </td>
        </tr>
</table>

<h2><a id="route_emitter"></a>Diego router-emitter Metrics</h2>

<h3><a id="RouteEmitterSyncDuration"></a>Route Emitter Time to Sync</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>route_emitter.RouteEmitterSyncDuration<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in ns that the active route-emitter took to perform its synchronization pass. 
                    <br><br>
                    <strong>Use</strong>: Increases in this metric indicate that the route emitter may have 
                    trouble maintaining an accurate routing table to broadcast to the Gorouters. 
                    Tune alerting values to your deployment based on historical data and adjust based on observations over time. 
                    The suggested starting point is &ge; 10 for the yellow threshold and &ge; 20 for the critical threshold. 
                    Pivotal has observed on its Pivotal Web Services deployment that above 20 s, the BBS may be failing.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ns)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 15 minutes divided by 1,000,000,000</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        <ol>
                        <li>
                        Investigate the route_emitter and Diego BBS logs for errors.
                        </li>
                        <li>
                        Verify that app routes are functional by making a request to an app, pushing an app and pinging it, or if applicable, checking that your smoke tests have passed.
                        </li>
                        </ol>
                </td>
        </tr>
</table>

<h3><a id="ConsulDownMode"></a>Consul Up or Down</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>route_emitter.ConsulDownMode<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Consul emits its health status periodically. <code>0</code> means the system is healthy, and <code>1</code> means that 
                    route emitter detects that Consul is down.
                    <br><br>
                    <strong>Use</strong>: During an upgrade, this can indicate that Consul is down.
                    Loss of the Consul cluster results in apps becoming unavailable 
                    because their routes were pruned from the routing table.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float, 0-1)<br>
                 <strong>Frequency</strong>: Periodically, emission should be constant on a running, Diego-based, PCF deployment<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em><br>
                <strong>Red critical</strong>: Not equal to 0 </td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>During upgrade, reduce the Consul server nodes to 1 and then upgrade.</li>
                        <li>Outside of an upgrade, consider disaster recovery for Consul.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h2><a id="gorouter"></a> Gorouter Metrics</h2>

<h3><a id="total_requests"></a>Router Throughput</h3>

<table>
   <tr><th colspan="2" style="text-align: center;"><br>gorouter.total_requests<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>The lifetime number of requests completed by the Gorouter VM 
          <br><br>
          <strong>Use</strong>: Provides insight into the overall traffic flow through a deployment. 
          For performance and capacity management, consider this metric a measure of router throughput 
          and convert it to requests-per-second, by looking at the delta value of `gorouter.total_requests` and deriving back to 1s, or <code>sum_over_all_indexes(gorouter.total_requests.delta)/5</code>.
          This helps you see trends in the throughput rate that indicate a need to scale the Gorouter.
          Use the trends you observe to tune the threshold alerts for this metric. 
          <br><br>
          <strong>Origin</strong>: Doppler/Firehose<br>
          <strong>Type</strong>: Counter (Integer)<br>
          <strong>Frequency</strong>: 5 s<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over the last 5 minutes of the derived per second calculation</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br>
          <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>For optimizing the Gorouter, consider the requests-per-second
          derived metric in the context of router latency and Gorouter VM CPU utilization. From performance and load testing of the gorouter, Pivotal has observed that at approximately 2500 requests per second, latency can begin to increase. 
          <br><br>
          To increase throughput and maintain low latency, 
          scale the Gorouters either horizontally or vertically 
          and watch that the <a href="#cpu.user">system.cpu.user</a> metric for the Gorouter stays in the suggested range of 60-70% CPU Utilization.
</tr>
</table>

<h3><a id="latency"></a>Router Handling Latency</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.latency<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The time in milliseconds that the Gorouter takes to handle requests to its app endpoints. 
                    This is the average round trip response time to an app, which includes router handling. 
                    <br><br>
                    <strong>Use</strong>: Indicates how Gorouter jobs in PCF are impacting overall app responsiveness. 
                    Latencies above 100 ms can indicate problems with the network, misbehaving apps, 
                    or the need to scale the Gorouter itself due to ongoing traffic congestion. 
                    An alert value on this metric should be tuned to the specifics of the deployment and its underlying network considerations; 
                    a suggested starting point is 100 ms. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ms)<br>
                 <strong>Frequency</strong>: Emitted per Gorouter request,
                         emission should be constant on a running deployment
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 30 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>Extended periods of high latency can point to several factors. 
                    The Gorouter latency measure includes network and app latency impacts as well.<br><br>
                    <ol>
                        <li>First inspect logs for network issues and indications of misbehaving apps.</li>
                        <li>If it appears that the Gorouter needs to scale due to ongoing traffic congestion, 
                            do not scale on the latency metric alone. You should also look at the CPU utilization of the Gorouter VMs and keep it within a maximum 60-70% range.</li>
                        <li>Resolve high utilization by scaling the Gorouter.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="mssincelastregistryupdate"></a>Time Since Last Route Register Received</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.ms_since_last_registry_update<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Time in milliseconds since the last route register was received.
                        <br><br>
                <strong>Use</strong>: Indicates if routes are not being registered to apps correctly. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float in ms)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum over the last 5 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
                <strong>Red critical</strong>: &gt; 30,000<br>
                        This threshold is suitable for normal platform usage. It alerts if it has been 
                        at least 30 seconds since the Gorouter last received a message from an app.</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Search the Gorouter and route_emitter logs for connection issues to NATS.</li>
                        <li>Check the BOSH logs to see if the NATS, Gorouter, or route_emitter VMs are failing.</li>
                        <li>Look more broadly at the health of all VMs, particularly Diego-related VMs.</li>
                        <li>If problems persist, pull the Gorouter and route_emitter logs and contact 
                            Pivotal Support to say there are consistently long delays in route registry.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="bad_gateways"></a>Router Error: 502 Bad Gateway</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.bad_gateways<br><br></th></tr>
     <tr>
         <th width="25%">Description</th>
            <td>The lifetime number of bad gateways, or 502 responses, from Gorouter itself.<br>
                The Gorouter emits a 502 bad gateway error when it has a route in the routing table 
                and, in attempting to make a connection to the backend, finds that the backend does not exist.
                    <br><br>
                    <strong>Use</strong>: Indicates that route tables might be stale.
                    Stale routing tables suggest an issue in the route register management plane, 
                    which indicates that something has likely changed with the locations of the containers. 
                    Always investigate unexpected increases in this metric.
                    <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Count (Integer, Lifetime)<br>
                 <strong>Frequency</strong>: 5 s<br>
             </td>
     </tr>
     <tr>
                <th>Recommended measurement</th>
                <td>Maximum delta per minute over a 5-minute window</td>
     </tr>
     <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
     </tr>
     <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Look in the Gorouter and route_emitter logs for connection issues to NATS.</li>
                        <li>Check the BOSH logs to see if the NATS, Gorouter, or route_emitter VMs are failing.</li>
                        <li>Look broadly at the health of all VMs, particularly Diego-related VMs.</li>
                        <li>If problems persist, pull Gorouter and route_emitter logs and contact 
                            Pivotal Support to say there has been an unusual increase in Gorouter bad gateway responses.</li>
                    </ol>
                    </td>
     </tr>
</table>

<h3><a id="responses.5xx"></a>Router Error: Server Error</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.responses.5xx<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The lifetime number of requests completed by the Gorouter VM for HTTP status family 5xx, server errors.
                    <br><br>
                    <strong>Use</strong>: A repeatedly crashing app is often the cause of a big increase in 5xx responses.
                    However, response issues from apps can also cause an increase in 5xx responses.
                    Always investigate an unexpected increase in this metric.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum delta per minute over a 5-minute window</code></td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Look for out-of-memory errors and other app-level errors.</li>
                        <li>As a temporary measure, ensure that the troublesome app is scaled to more than one instance.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="total_routes"></a>Number of Gorouter Routes Registered</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>gorouter.total_routes<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The current total number of routes registered with the Gorouter
                    <br><br>
                    <strong>Use</strong>: Indicates uptake and gives a picture of the overall 
                    growth of the environment for capacity planning.
            <br><br>
        Pivotal also recommends alerting on this metric if the number of routes falls outside of the normal
                    range for your deployment. For example, dramatic increases in the total routes outside of expected business 
                    events might point to a denial-of-service attack. 
                    Or, dramatic decreases in this metric volume might indicate a problem with 
                    the route registration process, such as an
                    app outage or that something in the route register management plane has failed.
            <br><br>
            If visualizing these metrics on a dashboard, <code>gorouter.total_routes</code> can be helpful by visualizing dramatic drops. However, for alerting purposes, the metric <code>gorouter.ms_since_last_registry_update</code> is more valuable for quicker identification of Gorouter issues. Alerting thresholds for <code>gorouter.total_routes</code> should focus on dramatic increases/decreases out of expected range.   
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Gauge (Float)<br>
                 <strong>Frequency</strong>: 30 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>5-minute average of the per second delta</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>For capacity needs, scale up or down the Gorouter VMs as necessary.</li>
                        <li>For significant drops in current total routes, see the 
                           <a href="#mssincelastregistryupdate"> <code>gorouter.ms_since_last_registry_update</code></a>
                            metric value for additional context.</li>
                        <li>Look at the Gorouter and route_emitter logs to look for connection issues to NATS.</li>
                        <li>Check the BOSH logs to see if the NATS, Gorouter or route_emitter VMs are failing.</li>
                        <li>Look broadly at the health of all VMs, particularly Diego-related VMs.</li>
                        <li>If problems persist, pull the Gorouter and route_emitter logs and contact Pivotal Support.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h2><a id="doppler-server"></a> Doppler Server Metrics</h2>

<h3><a id="listeners.receivedEnvelopes"></a>Firehose Throughput</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>DopplerServer.listeners.totalReceivedMessageCount
<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The total number of messages received across 
                    all Doppler listeners: UDP, TCP, TLS, and GRPC.
                    <br><br>
                    <strong>Use</strong>: Provides insight into how much traffic the logging system handles. This metric is an indicator of logging consistency.
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum delta per minute
                    over a 5-minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: Dynamic<br>
                <strong>Red critical</strong>: Dynamic</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Scale up the Firehose log receiver and Dopplers on consistent upward trends.<br>
                        Pivotal recommends that you do not scale down these components on flat or downward delta trends because unexpected spikes in throughput can cause log loss if not scaled appropriately.
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="deriveddopplerserver.doppler"></a>Firehose Dropped Messages</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>DopplerServer.doppler.shedEnvelopes 
                       + DopplerServer.TruncatingBuffer.totalDroppedMessages<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>The lifetime total number of messages 
                    intentionally dropped by Doppler due to back pressure.<br><br>
                    In PCF v1.10, Pivotal recommends combining both of these metrics to present a full picture of dropped messages.
                    However, the metric <code>DopplerServer.TruncatingBuffer.totalDroppedMessages</code> is 
                    being transitioned to <code>DopplerServer.doppler.shedEnvelopes</code>.  
                    In PCF v1.10, truncating buffer usage is approximately 5% of all possible traffic, primarily syslog drains.
                    <br><br>
                    <strong>Use</strong>: This metric is an important indicator of consistent logging. Set an alert to indicate if there is too much traffic coming in to the Dopplers 
                     or if the Firehose consumers are not keeping pace. 
                     Both issues result in dropped messages. 
                 <br><br>
                 <strong>Origin</strong>: Doppler/Firehose<br>
                 <strong>Type</strong>: Counter (Integer)<br>
                 <strong>Frequency</strong>: 5 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Maximum delta per minute
                    over a 5-minute window</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 5<br>
                <strong>Red critical</strong>: &ge; 10</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Scale up the Firehose log receiver and Dopplers.
                    </td>
        </tr>
</table>

<h2><a id="bosh"></a> System (BOSH) Metrics</h2>

<h3><a id="healthy"></a>VM Health</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.healthy<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td><code>1</code> means the system is healthy, and <code>0</code> means the system is not healthy.
                <br><br>
                <strong>Use</strong>: This is the most important BOSH metric to monitor. 
                 It indicates if the VM emitting the metric is healthy.
                 Review this metric for all VMs to estimate the overall health of the system.
            <br><br>
            Multiple unhealthy VMs signals problems with the underlying IAAS layer.
                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (Float, 0-1)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: <em>N/A</em> <br>
                <strong>Red critical</strong>: &lt; 1</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                        Investigate CF logs for the unhealthy component(s).
                    </td>
        </tr>
</table>

<h3><a id="mem.percent"></a>VM Memory Used</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.mem.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>System Memory — Percentage of memory used on the VM 
                        <br><br>
                 <strong>Use</strong>: Set an alert and investigate if the free RAM is low over an extended period. 
                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 10 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80%<br>
                <strong>Red critical</strong>:&ge; 90%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    The response depends on the job the metric is associated with. If appropriate, scale affected jobs out and monitor for improvement.
                </td>
        </tr>
</table>

<h3><a id="disk.system.percent"></a>VM Disk Used</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.system.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>System disk — Percentage of the system disk used on the VM
                        <br><br>
                 <strong>Use</strong>: Set an alert to indicate when the system disk is almost full.
                 <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 30 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80%</br>
                <strong>Red critical</strong>: &ge; 90%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    Investigate what is filling the jobs system partition. <br>
                    This partition should not typically fill because BOSH deploys jobs to use ephemeral and persistent disks.
                    </td> 
        </tr>
</table>

<h3><a id="disk.ephemeral.percent"></a>VM Ephemeral Disk Used</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.ephemeral.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Ephemeral disk — Percentage of the ephemeral disk used on the VM
                <br><br>
               <strong>Use</strong>: Set an alert and investigate if the ephemeral 
                disk usage is too high for a job over an extended period.
                <br><br>
                <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                <strong>Type</strong>: Gauge (%)<br>
                <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 30 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80%</br>
                <strong>Red critical</strong>: &ge; 90%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Run <code>bosh vms --details</code> to view jobs on affected deployments.</li>
                        <li>Determine cause of the data consumption, and, if appropriate, 
                            increase disk space or scale out the affected jobs.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="disk.persistent.percent"></a>VM Persistent Disk Used</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.disk.persistent.percent<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>Persistent disk — Percentage of persistent disk used on the VM
                        <br><br>
                <strong>Use</strong>: Set an alert and investigate further if the persistent disk usage for a job is too high 
                over an extended period.
                        <br><br>
                 <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                 <strong>Type</strong>: Gauge (%)<br>
                 <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 30 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 80%<br>
                <strong>Red critical</strong>: &ge; 90%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Run <code>bosh vms --details</code> to view jobs on affected deployments.</li>
                        <li>Determine cause of the data consumption, and, if appropriate, 
                            increase disk space or scale out affected jobs.</li>
                    </ol>
                    </td>
        </tr>
</table>

<h3><a id="cpu.user"></a>VM CPU Utilization</h3>

<table>
     <tr><th colspan="2" style="text-align: center;"><br>system.cpu.user<br><br></th></tr>
        <tr>
                <th width="25%">Description</th>
                <td>CPU utilization — The percentage of CPU spent in user processes
                <br><br>
                <strong>Use</strong>: Set an alert and investigate further if the CPU utilitization is
                too high for a job.<br><br>
                For monitoring Gorouter performance, CPU utilization of the Gorouter VM is 
                the recommended key capacity scaling indicator.
                For more information, see <a href="./key-cap-scaling.html#system.cpu.user">Gorouter Latency and Throughput</a>.
                <br><br>
                <strong>Origin</strong>: JMX Bridge or BOSH HM Forwarder<br>
                <strong>Type</strong>: Gauge (%)<br>
                <strong>Frequency</strong>: 60 s<br>
        </tr>
        <tr>
                <th>Recommended measurement</th>
                <td>Average over the last 5 minutes</td>
        </tr>
        <tr>
                <th>Recommended alert thresholds</th>
                <td><strong>Yellow warning</strong>: &ge; 85%<br>
                <strong>Red critical</strong>: &ge; 95%</td>
        </tr>
        <tr>
                <th>Recommended response</th>
                <td>
                    <ol>
                        <li>Investigate the cause of the spike.</li>
                        <li>If the cause is a normal workload increase, then scale up the affected jobs.</li>
                    </ol>
                    </td>
        </tr>
</table>

          <gcse:searchresults></gcse:searchresults>
<a id='repo-link' href='http://github.com/docs-content/tree/master/kpi.html.md.erb'>View the source for this page in GitHub</a>

      </main>
    </div>
  </div>
</div>

<div id="scrim"></div>

<div class="container">
  <footer class="site-footer-links">
    <div class="copyright">
  <a href='http://pivotal.io/privacy-policy'>Privacy Policy</a> | 
  <a href='http://pivotal.io/terms-of-use'>Terms of Use</a><br/>
  <a href='/'>Pivotal Documentation</a>
  &copy; 2017 <a href='http://pivotal.io'>Pivotal Software</a>, Inc. All Rights Reserved.
</div>
<div class="support">
  Need help? <a href="" target="_blank">Visit Support</a>
</div>

  </footer>
</div><!--end of container-->

</body>
</html>
